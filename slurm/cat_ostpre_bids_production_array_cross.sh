#!/bin/bash
# Jussi Tohka University of Eastern Finland, for runs in the UEF's "sampo" cluster 
#SBATCH --ntasks 1		# Number of taskS
#SBATCH --time 72:00:00		# Runtime
#SBATCH --mem-per-cpu=6000	# Reserve 6 GB RAM for a task, this is the memory required for a single subject
#SBATCH --partition serial	# Partition to submit
#SBATCH --job-name ostpre_slurm  
#SBATCH --output /research/users/justoh/slurm_logs/ostpre_cat_logs/ostpre1998_%A_%a.err    # Standard output goes to here
#SBATCH --error  /research/users/justoh/slurm_logs/ostpre_cat_logs/ostpre1998_%A_%a.out    # Standard error goes to here
#SBATCH --array=1-167%50     # Array range and number of simultanous jobs, note that these indexes refer to
                               # lines of the subjid.txt file in the ADNIDIR
                               # subjid.txt should be kept unchanged and contain all the subject indexes
                               # this allows for keeping track what subjects have been already processed based on log-files
                               # ,i.e., a log file always has an index corresponding to the line number in the subjid.txt file.
                               # subjid.txt can be generated by 
                               # ls -d sub-* > subjid.txt
                               
CATDIR=/research/users/justoh/matlab/spm12/spm12/toolbox/cat12
BIDSDIR=/research/groups/ostpre/ostpre_bids/bids
ROWINDEX=$((SLURM_ARRAY_TASK_ID+1998)) # to bypass MaxArraySize limit
SUBJ=$(sed -n "$ROWINDEX"p ${BIDSDIR}/subjid.txt)
DONEDIR=/research/users/justoh/slurm_logs/ostpre_cat_done  # Optional, for additional monitoring 


module load matlab # load modules

cd ${CATDIR}
bash cat_batch_bids_cross_force.sh ${BIDSDIR}/${SUBJ}
echo  "Done" >> ${DONEDIR}/${SUBJ}_${ROWINDEX}_cross.txt  # Optional: write a separate log-file to allow easy monitoring  
